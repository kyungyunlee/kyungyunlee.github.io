<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Naive Bayes | Kyung Yun Lee</title> <meta name="author" content="Kyung Yun Lee"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%B6&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://kyungyunlee.github.io/study/2020/04/04/naive_bayes.html"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Kyung Yun </span>Lee</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Naive Bayes</h1> <p class="post-meta">April 4, 2020</p> <p class="post-tags"> <a href="/blog/2020"> <i class="fas fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/tag/ml"> <i class="fas fa-hashtag fa-sm"></i> ML</a>     ·   <a href="/blog/category/study"> <i class="fas fa-tag fa-sm"></i> study</a>   </p> </header> <article class="post-content"> <h4 id="naive-bayes">Naive Bayes</h4> <p>Naive Bayes algorithm is a supervised learning method using Bayes rule with conditional independence assumption over data attributes.</p> <p>First, Bayes rule says that \(P(Y \vert X)\) can be expressed by \(P(X \vert Y)\) and \(P(Y)\). It is particularly useful when \(P(Y \vert X)\) cannot be computed directly.</p> \[P(Y_k|X) = \dfrac{P(X|Y_k)P(Y_k)}{\sum_j P(X|Y_j)P(Y_j)}\] <h4 id="what-is-the-naive-part-of-naive-bayes-algorithm">What is the “naive” part of Naive Bayes algorithm</h4> <p>Naive refers to our assumption that if we have n attributes, \(X = &lt;X_1, X_2, ..., X_n&gt;\), then, these attributes are <strong>conditionally independent</strong> from one another.</p> <p>When we say “\(X_1\) is conditionally independent from \(X_2\) given \(Y\)”, we write it as below .</p> \[P(X_1|X_2,Y) = P(X_1|Y)\] <p>This means that \(X_1\) and \(X_2\) are conditionally independent. It is a critical assumption to make our algorithm tractable.</p> <p>Here is a perfect example from the CMU lecture. To predict thunder, it doesn’t matter if it’s raining or not, when we see lightning. Thunder is conditionally independent from rain, when lightning is observed.</p> \[P(thunder | rain, lightning) = P(thunder|lightning)\] <h4 id="why-do-we-need-to-be-naive">Why do we need to be naive?</h4> <p>What is the reason for talking about conditional independence? Let’s say we are training a Bayes classifier without this assumption.</p> <p>We need to have some training data to predict the parameters for \(P(Y)\) and \(P(X|Y)\). Say \(x_i\) is a \(n\)-dimensional vector representing $ n $ attributes. Below is a conditional joint probability, \(P(X_1, X_2, ..., X_n \vert Y)\).</p> \[\theta_{ij} \equiv P(X= x_i|Y=y_j)\] <p>Each of \(x_i\) can take \(2^n\) possible values. To be exact, we need \(2^n -1\), since summing over all \(x_i\)’s is 1, when we fix \(Y=1\). Because we have to estimate for both \(Y = 1\) and \(Y = 0\), we need \(2 * (2^n -1)\) in total. So then, what if we have 30 features? The number of parameters becomes over 1 billion. In other words, we need more than 1 billion data exampes to represent all joint probabilities…</p> <p>Conditional independence to the rescue!<br> We will only need \(2*n\) parameters, because now with conditional independence, we have</p> \[P(X_1,X_2,...X_n|Y) = \prod_i{P(X_i|Y)}\] <p>We no longer need to estimate the joint probability; we just need to figure out \(P(X_i \vert Y)\).<br> The number of computation has reduced from \(2*(2^n-1)\) to \(2*n\) (given 30 features, 1 billiont to 60).</p> <h4 id="extending-to-discrete-values-multinomial">Extending to discrete values (multinomial)</h4> <p>Now instead of each \(X_i\) taking 0 or 1, let’s say it can take \(j\) different discrete values. Same for \(Y\), now it can take \(K\) different discrete values. It’s a simple extension to the above.</p> <p>We need to estimate the probability that \(X_i\) will take \(j\)-th value, using MLE.</p> \[\theta_{ijk} = P(X= x_{ij}|Y=y_k) = \dfrac{\# D\{X=x_{ij} \wedge Y=y_k\}}{\#D\{Y=y_k\}}\] <p>Or MAP</p> \[\theta_{ijk} = P(X= x_{ij}| Y=y_k) = \dfrac{\# D\{X=x_{ij} \wedge Y=y_k\} + h}{\#D\{Y=y_k\} + hJ}\] <p>where \(h\) is number of halluciating examples.</p> <p>Also we need to estimate the prior probability of Y, using MLE.</p> \[\pi_k = P(Y=y_k) = \dfrac{\# D\{Y=y_k\}}{\#D}\] <p>or MAP.</p> \[\pi_k = P(Y=y_k) = \dfrac{\# D\{Y=y_k\} + h }{\#D+hK}\] <p>MAP can be preferred over MLE, when there is limited data, so that we can avoid zero probabilities.</p> <h4 id="extending-to-continuous-values">Extending to continuous values</h4> <p>When the input values are continuous, we will use a probability distribution to estimate \(P(X \vert Y)\) and \(P(Y)\), instead of counting the number of observations in the data. <br> Simplest way is to use the gaussian distribution. Our task becomes estimating the mean and the variance of each attribute \(X_i\) given that \(Y=y_k\), and of the prior \(Y\). I coded a simple classifier for sad music classification (described below).</p> <p>In real applications, additional assumptions can be added to reduce the number of parameters to estimate. For example, if we observe a common noise that is independent from the training objective, we can assume the variance of all the attributes and/or the priors are the same.</p> <h4 id="pseudo-code-for-training-a-naive-bayes-classifier">Pseudo code for training a naive bayes classifier</h4> <p>For each value of \(Y=y_k\) :</p> <ul> <li>estimate prior \(P(Y=y_k)\)</li> <li>For each value of \(x_{ij}\) : <ul> <li>estimate \(P(X=x_{ij} \vert Y=y_k)\)</li> </ul> </li> </ul> <h4 id="finally-sad-music-classifier-with-gaussian-naive-bayes">Finally! Sad music classifier with Gaussian Naive Bayes</h4> <p>I implemented a gaussian naive bayes classifier for sad music using audio feaures provided by <a href="https://developer.spotify.com/documentation/web-api/" rel="external nofollow noopener" target="_blank">Spotify Web API</a>. To make the data, I simply grabbed 100 songs each from a “happy” playlist (<a href="https://open.spotify.com/playlist/37i9dQZF1DXdPec7aLTmlC?si=_bMVBft5REu2ge_e4bIfbgand" rel="external nofollow noopener" target="_blank">“Happy Hits!”</a>) and a “sad” playlist (<a href="https://open.spotify.com/playlist/54ozEbxQMa0OeozoSoRvcL?si=P2ODWYZ1S-Wo_IkFaSX8_A" rel="external nofollow noopener" target="_blank">“Sad Songs”</a>).</p> <p>Specifically, I chose 5 attributes, ‘danceability’, ‘tempo’, ‘energy’, ‘valence’ and ‘loudness’. These values are obtained using <a href="https://github.com/plamere/spotipy" rel="external nofollow noopener" target="_blank">spotipy</a>, which is a python wrapper for Spotify Web API.<br> Below is a data distribution for each attribute, where blue indicates sad music and red indicates happy music.</p> <p><img src="/assets/dist.png" alt="distribution" width="800px"></p> <p>For each of the attributes, I calculate the mean and std for the gaussian distribution. <br> Here are few values :</p> <ul> <li>\(P(X = Danceability \vert Y = Sad)\) : \(\mu\) = 0.5505, \(\sigma\) = 0.1013</li> <li>\(P(X = Danceability \vert Y = Happy)\) : \(\mu\) = 0.6923, \(\sigma\) = 0.1233</li> <li>\(P(X = Valence \vert Y = Sad)\) : \(\mu\) = 0.3515, \(\sigma\) = 0.1908</li> <li>\(P(X = Valence \vert Y = Happy)\) : \(\mu\) = 0.5794, \(\sigma\) = 0.1815</li> </ul> <p>From this, we can calulate the probability of attribute $ X_i $ taking a certain value by using a normal probability density function.</p> \[p(x) = \dfrac{1}{\sigma \sqrt{2 \pi}} e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}\] <p>Prior probability is \(P(Y=Sad) = 0.5\) and \(P(Y=Happy) = 0.5\), since I used 30 songs for each class.</p> <p>Now, all we need to do when we have a new data observation \(X\) is to compute \(P(Y \vert X)\) using Bayes rule.</p> \[P(Y=Sad | X = [X_{danceability}, X_{tempo}, ...]) = P(Y=Sad) * gaussian\_prob(danceability\_stats, X_{danceability}) * gaussian\_prob(tempo\_stats, X_{tempo}) * ...\] \[P(Y=Happy | X = [X_{danceability}, X_{tempo}, ...]) = P(Y=Happy) * gaussian\_prob(danceability\_stats, X_{danceability}) * gaussian\_prob(tempo\_stats, X_{tempo}) * ...\] <p>From this two estimated probabilities, we take the larger value and say whether the given song is sad or happy. Naive bayes turns out to be pretty good. This yields around 93 % accuracy! A random chance is 50%. I have uploaded the code and data, so if anyone comes across this post and wants to play around, it is <a href="https://github.com/kyungyunlee/ml_python/tree/master/sad_music_naive_bayes" rel="external nofollow noopener" target="_blank">here on my github</a>.</p> <p>Coming up next is maybe logistic or linear regression.</p> <h4 id="reference">Reference</h4> <ul> <li>CMU Machine learning 2015 course by Tom Mitchell</li> <li><a href="https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/" rel="external nofollow noopener" target="_blank">https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/</a></li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Kyung Yun Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>