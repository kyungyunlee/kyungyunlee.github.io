<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kyungyunlee.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kyungyunlee.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-17T21:15:19+00:00</updated><id>https://kyungyunlee.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">MMSE denoiser and Tweedie’s Formula</title><link href="https://kyungyunlee.github.io/blog/2025/Tweedies/" rel="alternate" type="text/html" title="MMSE denoiser and Tweedie’s Formula"/><published>2025-12-06T00:00:00+00:00</published><updated>2025-12-06T00:00:00+00:00</updated><id>https://kyungyunlee.github.io/blog/2025/Tweedies</id><content type="html" xml:base="https://kyungyunlee.github.io/blog/2025/Tweedies/"><![CDATA[<p>MMSE is an estimator that gives us the expectation of the posterior.</p> <p>Let \(x\) be a clean signal and \(\tilde{x}\) a noisy version of that:</p> \[\begin{align} \begin{aligned} \tilde{x} &amp;= x + \sigma \epsilon \\ \epsilon &amp;\sim N(0, I) \end{aligned} \end{align}\] <p>then, the MMSE denoiser is</p> \[\hat{x} = \mathbb{E}[x|\tilde{x}] = \int x p(x|\tilde{x}) dx = \int x \frac{p(\tilde{x}|x)p(x)}{p(\tilde{x})} dx\] <p>From the setup of the problem, we know that \(p(\tilde{x}\vert x) \sim N(x, \sigma^2)\), thus the marginal distribution of \(p(\tilde{x})\) is</p> \[p(\tilde{x}) = \int p(\tilde{x}|x) p(x) dx.\] <p>As a result, we can rewrite the MMSE denoiser as the following:</p> \[\mathbb{E}[x|\tilde{x}] = \frac{\int x p(\tilde{x}|x)p(x) dx}{\int p(\tilde{x}|x) p(x) dx}\] <p>We can simplify this formula using some trick, because there are things like \(p(x)\), which is intractable. This trick is what leads to the Tweedie’s formula.</p> <p>First, we call the numerator \(N(\tilde{x}) = \int x p(\tilde{x} \vert x)p(x) dx\) and the denominator \(m(\tilde{x}) = p(\tilde{x}) = \int p(\tilde{x} \vert x) p(x) dx\).</p> <p>The trick is to take the derivative of the marginal \(p(\tilde{x}) = m(\tilde{x})\).</p> \[\frac{d}{d\tilde{x}}m(\tilde{x}) = \frac{1}{\sigma^2} \int (x-\tilde{x}) p(\tilde{x}|x) p(x)dx\] \[= \frac{1}{\sigma^2}(N(\tilde{x}) - \tilde{x} m(\tilde{x}))\] <p>Then, we can represent the numerator \(N(\tilde{x})\) in terms of this gradient:</p> \[N(\tilde{x}) = \tilde{x} m(\tilde{x}) + \sigma^2 \frac{d}{d\tilde{x}}m(\tilde{x})\] <p>Plugging these back into the MMSE formula, we get</p> \[\mathbb{E}[x|\tilde{x}] = \frac{\tilde{x} m(\tilde{x}) + \sigma^2 \frac{d}{d\tilde{x}}m(\tilde{x}) }{m(\tilde{x})}\] \[= \tilde{x} + \sigma^2 \frac{d}{d\tilde{x}} \text{log} m(\tilde{x})\] \[= \tilde{x} + \sigma^2 \frac{d}{d\tilde{x}} \text{log} p(\tilde{x}),\] <p>which is the Tweedie’s formula.</p> <p>As \(\frac{d}{d\tilde{x}} \text{log} p(\tilde{x}) = \nabla_{\tilde{x}} \text{log} p(\tilde{x})\), this formula tells us that you can denoise a noisy signal just by correcting the noisy signal with the score function.</p> <p>The paper by Efron talks about the motivation behind it. The example in the paper is to estimate the means of many random variables each from single noisy observations. If you happen to observe only very extreme values, something that occurs under selection bias, these observations are simply inflated by noise.<br/> Tweedie’s Formula corrects this inflation and allows you to estimate the debiased mean.</p> <h3 id="how-does-the-tweedies-formula-pop-up-in-diffusion-and-flow-models">How does the Tweedie’s formula pop up in diffusion and flow models?</h3> <p>Tweedie’s formula connects the MMSE denoiser and the score function. This connection is actually very fundamental in diffusion and flow models. Instead of learning the actual prior distribution directly, these models instead learn the conditional score function or conditional vector field, which is possible by the noising and denoising framework.</p> <p>At each time step of diffusion/flow model, the denoising step is essentially the Tweedie’s formula. Thus, by learning the score function or the vector field, the model can express the denoising process.</p> \[\mathbb{E}[x_0 | x_t]= x_t + \sigma_t^2 \nabla_{x_t} \text{log} p_{t}(x_t)\] <p>References</p> <ul> <li>Efron, Bradley. “Tweedie’s formula and selection bias.” Journal of the American Statistical Association 106.496 (2011): 1602-1614.</li> <li>Lai, Chieh-Hsin, et al. “The principles of diffusion models.” arXiv preprint arXiv:2510.21890 (2025).</li> </ul>]]></content><author><name></name></author><category term="statistics"/><category term="statistics,"/><category term="diffusion"/><summary type="html"><![CDATA[Diffusion Models Study]]></summary></entry><entry><title type="html">SDEs and Fokker-Planck Equation</title><link href="https://kyungyunlee.github.io/blog/2025/SDEs-Fokker-Planck/" rel="alternate" type="text/html" title="SDEs and Fokker-Planck Equation"/><published>2025-12-02T00:00:00+00:00</published><updated>2025-12-02T00:00:00+00:00</updated><id>https://kyungyunlee.github.io/blog/2025/SDEs-Fokker-Planck</id><content type="html" xml:base="https://kyungyunlee.github.io/blog/2025/SDEs-Fokker-Planck/"><![CDATA[<p>Demystifying and connecting</p> <ul> <li>Stationary and non-stationary SDEs</li> <li>Langevin dynamics, MCMC</li> <li>Diffusion models</li> <li>Fokker-Planck Equation</li> </ul> <h3 id="stationary-vs-non-stationary-sdes">Stationary vs. Non-stationary SDEs</h3> <p>Stationary SDE means that the distribution doesn’t change over time under the SDE dynamics. So starting from \(x_0 \sim p(x)\), it will always gives \(x_t \sim p(x)\) for all t.</p> <p>Langevin SDE is an example of a stationary SDE with the stationary distribution \(p(x)\).</p> <p>Non-stationary SDE has changing distribution over time. The purpose of non-stationary SDE is not to sample, but to transform data distribution.</p> <p>Diffusion model’s forward process is an example of a non-stationary SDE.</p> <p>It took some time for me to clarify these different roles of SDEs while learning about diffusion and flow models.</p> <h3 id="langevin-sde-stationary-sde">Langevin SDE (stationary SDE)</h3> <p>Lagevin SDE is as the following:</p> \[dx_t = \nabla_x \text{log}p(x_t)dt + \sqrt{2} dw_t,\] <p>where \(w_t\) is a Brownian motion (or called a Wiener process), a non-differentiable stochastic process).</p> <p>It’s main purpose is to use SDE to sample from the target \(p(x)\). Note that the score function is not time-dependent; It is not \(\nabla_x \text{log}p_t(x_t)dt\). Thus, it is a sampling algorithm. This makes Langevin Dynamic a MCMC sampler. If some distribution is hard to sample from directly, you would use these sampling methods.</p> <p>Why is there the \(\sqrt{2}\) term?</p> <p>Due to the stationary property, the Langevin SDE ensures that at the end of the iterative sampling steps, you will get a sample from the target \(p(x)\). This is achieved only because of the magic coefficient \(\sqrt{2}\), which balances the drift and the noise term.</p> <p>Note: Mathematically, the Langevin SDE as it is has nothing to do with diffusion models. The connection arises as there is something called, annealed Langevin dynamics.</p> <h3 id="diffusion-model-sde-non-stationary-sde">Diffusion model SDE (non-stationary SDE)</h3> <p>Diffusion model’s forward SDE is as the following:</p> \[dx_t = -\frac{1}{2} \beta(t)x_tdt + \sqrt{\beta(t)} dw_t,\] <p>where \(\beta(t)\) describes how fast \(x_t\) is destroyed by adding noise (aka. noise schedule). From this equation, we see that the first term (drift term) is negative and therefore, it makes the mean of the final distribution to go to zero. The variance of the final distribution comes from the second term (noise term). As a result, the final distribution of the forward SDE is \(N(0, I_d)\).</p> <h3 id="fokker-planck-equation">Fokker-Planck Equation</h3> <p>SDEs in general gives us a “particle” view. It tells us about how each particles move in a dynamic system and there are many different possible trajectories that these particles can move because of randomness. But behind every SDE, there is an underlying distribution, be it stationary or non-stationary.</p> <p>The Fokker-Planck equation gives this “distributional” view. It describes how distributions change over time and ignores the detail of particles. As a result, all the different trajectories generated by an SDE correspond to the same Fokker-Planck equation.</p> <ul> <li>SDE : sample path</li> <li>Fokker-Planck : distribution of these sample paths over time</li> </ul> <p>A general SDE is of the form:</p> \[dx_t = f(x_t,t) dt + g(t) dw_t,\] <p>where \(f(x_t,t)dt\) is a drift term that tells which direction sample \(x_t\) should go and \(g(t)dw_t\) is a diffusion term that adds perturbation.</p> <p>Then, the corresponding pdf \(p_t(x)\) satisfies the Fokker-Planck equation:</p> \[\frac{\partial p_t(x)}{\partial t} = - \nabla \cdot (f(x,t)p_t(x)) + \frac{1}{2} \nabla^2 (g(t)^2p_t(x)).\] <p>As in the SDE, the first term is here can also be interpreted as a drift term. \(\nabla \cdot\) is a divergence operator, which measures how much probability is entering or leaving at x(t) (equivalent to \(\text{Tr}(\nabla (f(x,t)p_t(x)))\)). It tells us if the density increases or decreases. The second term is a diffusion term, which tells how fast or slow the distribution spreads over time. \(\nabla^2\) is the Laplacian operator, which will lead \(p_t(x)\) to spread out. \(g(t)\) tells you how strong the diffusion is.</p> <h3 id="fokker-planck-to-sdes">Fokker-Planck to SDEs</h3> <p>For stationary SDE, \(\frac{\partial p_t(x)}{\partial t}=0\).<br/> We can see that for the Langevin SDE, by setting \(f(x,t) = \nabla_x \text{log} p(x_t)\) and \(g(t)=\sqrt{2}\):</p> \[\frac{\partial p_t(x)}{\partial t} = - \nabla \cdot (p_t(x) \nabla_x \text{log} p(x)) + \nabla^2 p_t(x) = 0.\] <p>Thus, \(p_t(x)\) does not change over time.</p> <p>For diffusion forward SDE, \(f(x,t) = -\frac{1}{2} \beta(t)x_t\) and \(g(t)=\sqrt{\beta(t)}\):</p> \[\frac{\partial p_t(x)}{\partial t} = \nabla \cdot (\frac{1}{2} \beta(t)x_t p_t(x)) + \frac{1}{2} \beta(t)\nabla^2p_t(x) \neq 0.\] <p>This shows why during the diffusion forward process, \(p_t(x)\) becomes more and more Gaussian over time.</p> <p><strong>References</strong></p> <ul> <li>Lai, Chieh-Hsin, et al. “The principles of diffusion models.” arXiv preprint arXiv:2510.21890 (2025).</li> </ul> ]]></content><author><name></name></author><category term="statistics"/><category term="statistics,"/><category term="diffusion"/><summary type="html"><![CDATA[Diffusion Models Study]]></summary></entry><entry><title type="html">DDPM, EBM and NCSN</title><link href="https://kyungyunlee.github.io/blog/2025/DDPM-NCSN/" rel="alternate" type="text/html" title="DDPM, EBM and NCSN"/><published>2025-12-01T00:00:00+00:00</published><updated>2025-12-01T00:00:00+00:00</updated><id>https://kyungyunlee.github.io/blog/2025/DDPM-NCSN</id><content type="html" xml:base="https://kyungyunlee.github.io/blog/2025/DDPM-NCSN/"><![CDATA[<p>As we all know, there has been a rapid evolution of distribution- learning models. For me, it has been and still is a challenge to see the connection between them. Only looking at the final objective, they look like they have different goals. What helped me to get a birds-eye view is remembering that all these models are trying to learn the data distribution, and the most basic objective for that is maximizing the likelihood.</p> <p>DDPMs and EBMs’ objectives look different at first glance, but both objectives initially arise from MLE.</p> <p>For DDPM, maximizing likelihood leads to the ELBO. As the forward and reverse transitions are Gaussian, the ELBO simplifies to an MSE loss of the noise (or mean) prediction at each time step.</p> <p>For EBMs, maximizing the likelihood leads to minimizing the expected energy on data while maximizing entropy through the normalizing constant. However, the intractability of the normalizing constant motivates working with the gradient of the log-likelihood instead. Therefore came the idea of using score functions instead of the densities, as score functions are sufficient to describe the underlying density. So if you have learned a correct score function, you will obtain the right density function.</p> <p>As the idea of score matching came about, NCSN directly uses this idea to train a score-matching model, bypassing the energy function.</p>]]></content><author><name></name></author><category term="statistics"/><category term="statistics,"/><category term="diffusion"/><summary type="html"><![CDATA[Diffusion Models Study]]></summary></entry><entry><title type="html">Energy-based models and Score matching models</title><link href="https://kyungyunlee.github.io/blog/2025/EBM-Score/" rel="alternate" type="text/html" title="Energy-based models and Score matching models"/><published>2025-11-29T00:00:00+00:00</published><updated>2025-11-29T00:00:00+00:00</updated><id>https://kyungyunlee.github.io/blog/2025/EBM-Score</id><content type="html" xml:base="https://kyungyunlee.github.io/blog/2025/EBM-Score/"><![CDATA[<p>Energy-based models represent the pdf with an energy function, \(E_\phi(x)\). Basically, the ideally learned EBMs will tell us that higher probability data have lower energy.</p> \[p_\phi(x) = \frac{e^{-E_\phi(x)}}{Z_\phi}, Z_\phi = \int e^{-E_\phi(x)} dx\] <p>As usual, we would like to use the maximum likelihood to train EBMs:</p> \[L_{\text{MLE}}(\phi) = \mathbb{E}_{x \sim p_{\text{data}}(x)}[\text{log} p_\phi(x)] = - \mathbb{E}_{x \sim p_{\text{data}}(x)}[E_\phi(x)] - \text{log} Z_\phi\] <p>The first term tells us that the goal is to minimize the expected energy of observed data, i.e. maximize their probability. The second term is a regularizer that related to the entropy.</p> <p>Entropy of the model is: \(H(p_\phi) = -\mathbb{E}_{x \sim p_\phi}[\text{log} p_\phi(x)] = \mathbb{E}_{x \sim p_\phi}[E_\phi(x)] + \text{log} Z_\phi.\)</p> <p>As \(Z_\phi\) is a normalizing constant, if it increases, it means that the probability will generally decrease, i.e. more evenly distributed across space. This means that the overall entropy increases. This brings an effect of improved mode coverage.</p> <h3 id="challenge-in-ebms">Challenge in EBMs</h3> <p>Directly optimizing MLE in this way is a challenge, as \(Z_\phi\) is intractable. We can get away from this by playing in the gradient space as by taking the gradient of the log probability, the normalizing constant disappears. So now, comes the score function.</p> <p>Score function is a general term that refers to the gradient of the log probability. It tells us the direction of higher probability. It describes the vector field.</p> \[s(x) = \nabla_x \text{log} p(x)\] <p>If we plug in the energy function here, we get the following without the normalizing constant.</p> \[s_\phi(x) = \nabla_x \text{log} p_\phi(x) = - \nabla_x E_\phi(x)\] <p>So, instead of training EBMs to maximize the likelihood, we can train it by matching the model score to the true score function, \(s(x)\).</p> \[L_{SM}(\phi) = \frac{1}{2}\mathbb{E}_{x\sim p_\text{data}}|| s_\phi(x) - s(x) ||_2^2 \\ = \frac{1}{2}\mathbb{E}_{x\sim p_\text{data}}|| \nabla_x \text{log} p_\phi(x) - \nabla_x \text{log} p_{\text{data}}(x)||_2^2\] <p>As the true score function is not possible to compute, Hyvarinen et al. have shown that there is an equivalent form so that the objective only depends on \(s_\phi\).</p> \[L_{SM}(\phi) = \mathbb{E}_{x\sim p_\text{data}}[ \text{Tr}(\nabla_x s_\phi(x)) + \frac{1}{2} || s_\phi(x) ||_2^2]\] <p>Minimizing the second term simply tells us that the score should be small for highly probable data (high probability region should have 0 gradient (maximum)). Thus it is at best 0. This leads to the goal of making first term negative. It is the divergence term as it is the trace of the second derivative. Negative divergence means that around the high probability points, the vector field should point inwards, i.e. is a sink.</p> <h3 id="from-energy-based-models-to-score-based-models">From energy-based models to score-based models</h3> <p>Jacobian is expensive to compute. So came the idea that instead of bothering about energies to just directly work with scores. Can’t we just train a neural network that predicts score functions instead of energy functions?</p> <p>Yes, but the problem remains that the true score function is intractable. The idea to overcome this is by injecting known amount of noise to data and working with the “noisy” data distribution and conditionals.</p> \[\tilde{x} = x + N(0, \sigma^2I)\] \[p_\sigma(\tilde{x}) = \int p_\sigma(\tilde{x}|x) p_{\text{data}}(x)dx\] \[L_{SM}(\phi) = \frac{1}{2}\mathbb{E}_{\tilde{x} \sim p_\sigma}[|| s_\phi(\tilde{x}) - \nabla_{\tilde{x}} \text{log} p_\sigma(\tilde{x})||_2^2]\] <p>Using the conditioning technique, we can avoid the gradient of the marginal and just work with the conditional. The difference is only a constant, thus optimizing the conditional is equivalent to optimizing with the marginal distribution.</p> \[L_{DSM}(\phi) = \frac{1}{2}\mathbb{E}_{\tilde{x} \sim p_\sigma(\cdot | x), x \sim p_\text{data}}[|| s_\phi(\tilde{x}) - \nabla_{\tilde{x}} \text{log} p_\sigma(\tilde{x}|x)||_2^2]\] <p>The conditional is very easy to compute as we have designed the noisy version ourselves. For instance, for a Gaussian noise: \(p_\sigma(\tilde{x} \vert x) = N(\tilde{x}; x, \sigma^2I)\). The conditional score is then \(\nabla_{\tilde{x}} \text{log} p_\sigma(\tilde{x}\vert x) = -\frac{\tilde{x} - x}{\sigma^2}\).</p> <p>Thus, the objective simplifies in this case to:</p> \[L_{DSM}(\phi) = \frac{1}{2}\mathbb{E}_{\tilde{x} \sim p_\sigma(\cdot | x), x \sim p_\text{data}}[|| s_\phi(\tilde{x}) + \frac{\tilde{x} - x}{\sigma^2} ||_2^2].\] <p><strong>References</strong></p> <ul> <li>Lai, Chieh-Hsin, et al. “The principles of diffusion models.” arXiv preprint arXiv:2510.21890 (2025).</li> </ul>]]></content><author><name></name></author><category term="statistics"/><category term="statistics,"/><category term="diffusion"/><summary type="html"><![CDATA[Diffusion Models Study]]></summary></entry><entry><title type="html">Maximum Likelihood and KL Divergence</title><link href="https://kyungyunlee.github.io/blog/2025/ML_KL/" rel="alternate" type="text/html" title="Maximum Likelihood and KL Divergence"/><published>2025-11-27T00:00:00+00:00</published><updated>2025-11-27T00:00:00+00:00</updated><id>https://kyungyunlee.github.io/blog/2025/ML_KL</id><content type="html" xml:base="https://kyungyunlee.github.io/blog/2025/ML_KL/"><![CDATA[<p>The ultimate purpose of the generative model is generate new samples that look like real data. This means that we would need to know the probability distribution of data, \(p_{\text{data}}(x)\). However, this distribution is an oracle distribution, which is complex and practically impossible to know. For images, it would be the distribution of entire images in the world. Therefore, the goal of generative model \(p_\phi(x)\) is to approximate it, so that sampling becomes possible.</p> \[p_\phi(x) \approx p_{\text{data}}(x)\] <h3 id="maximum-likelihood-estimation-mle">Maximum likelihood Estimation (MLE)</h3> <p>Here I give a short review of MLE. Maximum likelihood estimator tries to find the best parameter which maximizes the probability of observing the dataset. The general formula for MLE is:</p> \[\phi^* = \text{argmax}_\phi \mathbb{E}_{x \sim p_{\text{data}}}[\text{log} p_\phi(x)],\] <p>where it is common to see this notation as well \(p(x \vert \phi)=p_\phi(x)\).</p> <p>Recall that the Bayes’ rule of the posterior is \(p(\phi \vert x) = \frac{p(x \vert \phi)p(\phi)}{p(x)}\). MLE ignores the prior of \(\phi\) and just maximizes the likelihood.</p> <p>I think here comes a bit of terminology confusion (at least for me). When talking about evidence lower bound (ELBO), the term “evidence” actually refers to the marginal likelihood of the data:</p> <p>\(p_\phi(x) = \int p_\phi(x,z) dz = \int p_\phi(x\vert z)p(z)dz\).</p> <p>as this appears in Bayes’ rule as:</p> \[p(z | x) = \frac{p_\phi(x|z)p(z)}{p_\phi(x)}\] <p>where \(p_\phi(x)\) appears as the evidence (aka. marginal likelihood).</p> <p>But, note that this is the Bayes’ formula for the latent variable model. \(p_\phi(x)\) has another layer as it is parametrized by \(\phi\), thus in terms of the parameter, \(p_\phi(x)\) is the data likelihood as shown above.</p> <h3 id="kl-divergence-and-mle">KL Divergence and MLE</h3> <p>So the generative model’s goal is to approximate the true data distribution. We can build an objective function based on the KL divergence, which compares the two distribution in terms of their information.</p> \[D_{KL}(p_{\text{data}} || p_\phi) = \mathbb{E}_{x \sim p_{\text{data}}}[\text{log} p_{\text{data}}(x) - \text{log}p_\phi(x)]\] <p>If we try to minimize this KL divergence with respect to \(\phi\), the term \(\mathbb{E}_{x \sim p_{\text{data}}}[\text{log}p_{\text{data}}(x)]\) becomes a constant. Thus, it boils down to maximizing the likelihood.</p> \[\text{min}_\phi D_{KL}(p_{\text{data}}||p_\phi) \Longleftrightarrow \text{max}_\phi \mathbb{E}_{x \sim p_{\text{data}}}[\text{log}p_\phi(x)]\] <p><strong>References</strong></p> <ul> <li>Lai, Chieh-Hsin, et al. “The principles of diffusion models.” arXiv preprint arXiv:2510.21890 (2025).</li> </ul>]]></content><author><name></name></author><category term="statistics"/><category term="statistics,"/><category term="diffusion"/><summary type="html"><![CDATA[Diffusion Models Study]]></summary></entry><entry><title type="html">Evidence Lower Bound (ELBO)</title><link href="https://kyungyunlee.github.io/blog/2025/elbo/" rel="alternate" type="text/html" title="Evidence Lower Bound (ELBO)"/><published>2025-11-27T00:00:00+00:00</published><updated>2025-11-27T00:00:00+00:00</updated><id>https://kyungyunlee.github.io/blog/2025/elbo</id><content type="html" xml:base="https://kyungyunlee.github.io/blog/2025/elbo/"><![CDATA[<p>In Bayes’ rule, evidence is another name for the marginal likelihood of the data. It is called as “evidence”, because it measures how well the data supports the model.</p> \[p(z|x) = \frac{p(x|z)p(z)}{\color{red}{p(x)}}\] <p>where \(p(x) = \int p(x,z) dz = \int p(x\vert z)p(z)dz\).</p> <p>The ultimate goal of many likelihood-based generative models is to maximize this. However, this marginal likelihood is most of the time intractable due to the integral over all possible latent variable \(z\). Therefore, ELBO, which is a tractable bound, comes as a useful tool in designing optimizers. Since we cannot maximize \(\text{log}p_\phi(x)\) directly, we instead maximize it’s lower bound.</p> \[\text{log}p_\phi(x) \geq L_{\text{ELBO}}(\theta, \phi; x)\] <p>I wrote the above formula for VAEs, as \(\theta\) is an encoder parameter, \(\phi\) is a decoder parameter. In case of DDPMs, encoder is fixed, so \(\theta\) doesn’t exist and \(\phi\) is the learnable decoder (denoiser) parameter.</p> <h3 id="deriving-elbo">Deriving ELBO</h3> <p>Deriving the formula for ELBO is actually quite simple. I do it here for VAEs. Let \(q_\theta(z|x)\) be an encoder of a VAE, which is an approximation of \(p_\phi(z|x)\). For DDPMs, this is just replaced by the known encoder \(p(x_{i-1}|x_i)\).</p> <p>First, we rewrite the marginal with a joint distribution and use \(q_\theta\).</p> \[\text{log}p_\phi(x) = \text{log} \int p_\phi(x,z) dz = log \int q_\theta(z|x) \frac{p_\phi(x,z)}{q_\theta(z|x)}dz \\ = \text{log} \mathbb{E}_{z \sim q_\theta(z|x)}[\frac{p_\phi(x,z)}{q_\theta(z|x)}]\] <p>Since log is concave function, the Jensen’s inequality tells us that \(\text{log} \mathbb{E}[Z] \geq \mathbb{E}[\text{log}Z]\) (think about drawing a straight line between two points in a concave function):</p> \[\text{log} \mathbb{E}_{z \sim q_\theta(z|x)}[\frac{p_\phi(x,z)}{q_\theta(z|x)}] \geq \mathbb{E}_{z \sim q_\theta(z|x)}[\text{log} \frac{p_\phi(x,z)}{q_\theta(z|x)}]\] <p>Then, just expanding the last equation gives the ELBO:</p> \[L_{ELBO} = \mathbb{E}_{z \sim q_\theta(z|x)}[\text{log}p_\phi(x|z)] - D_{KL}(q_\theta(z|x) || p(z))\] <p>where the KL divergence is the difference in the information of two distributions</p> \[D_{KL}(q_\theta(z|x) || p(z)) = \mathbb{E}_{z \sim q_\theta(z|x)}[\text{log}\frac{q_\theta(z|x)}{p(z)}].\] <p>The first term of ELBO is the likelihood of observing the data given the latent variable, which in other words, is the reconstruction term. The second KL divergence term pushes the encoder distribution \(q_\theta(z\vert x)\) to be close to the latent distribution \(p(z)\), which in many cases is a simple Gaussian.</p> <h3 id="elbo-for-ddpms">ELBO for DDPMs</h3> <p>Same as VAEs, the DDPMs’ training objective is also to maximize the marginal log-likelihood. Notable differences are that the encoder is a known distribution, since the “noising” process is small steps of adding Gaussian noise, and that the marginal log-likelihood is a joint distribution over all \(T\) annealing steps, instead of a single latent \(z\). Note that the DDPM notation is that \(x_0\) is the clean data and \(x_T\) is a noisy sample.</p> \[log p_\phi(x) = log \int p_\phi(x, x_{0:T})dx_{0:T}.\] <p>In the same way as in VAEs where we inserted \(q_\theta(z\vert x)\), here we insert known distribution of the forward process \(p(x_{0:T}\vert x)\), which is a joint distribution of all the noisy samples of x.</p> \[log p_\phi(x) = log \int p(x_{0:T}|x) \frac{p_\phi(x, x_{0:T})}{p(x_{0:T}|x)}dx_{0:T}\] <p>Expanding the above and using the Jensen’s inequality in the same way will give the following ELBO with 3 terms:</p> \[L_{\text{ELBO}}(x_0; \phi) = - D_{KL}(p(x_T|x_0)||p_{\text{prior}}(x_T)) + \mathbb{E}_{p(x_1|x_0)}[\text{log}p_\phi(x_0|x_1)] - \sum_{i=1}^T \mathbb{E}_{p(x_i|x_0)} [D_{KL}(p(x_{i-1}|x_i, x_0)||p_\phi(x_{i-1}|x_i))]\] <p>where the first one becomes close to zero if we add enough noise to make \(x_T\) completely noisy so that \(p(\cdot \vert x_0) \approx p_{\text{prior}}(\cdot)\), and the second one is a reconstruction/denoising term and the last term is the diffusion term, where we make sure that the true distribution of the denoising process \(p(x_{i-1}\vert x_i)\) is approximated with the learned denoiser \(p_\phi(x_{i-1}\vert x_i)\). Note that since \(p(x_{i-1}\vert x_i)\) is not tractable, conditional distribution is used as done commonly in most diffusion/flow models.</p> <p><strong>References</strong></p> <ul> <li>Lai, Chieh-Hsin, et al. “The principles of diffusion models.” arXiv preprint arXiv:2510.21890 (2025).</li> </ul>]]></content><author><name></name></author><category term="statistics"/><category term="statistics,"/><category term="diffusion"/><summary type="html"><![CDATA[Diffusion Models Study]]></summary></entry></feed>