<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Evidence Lower Bound (ELBO) | Kyung Yun Lee</title> <meta name="author" content="Kyung Yun Lee"> <meta name="description" content="Diffusion Models Study"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%B6&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://kyungyunlee.github.io/blog/2025/elbo/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Kyung Yun </span>Lee</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Evidence Lower Bound (ELBO)</h1> <p class="post-meta">November 27, 2025</p> <p class="post-tags"> <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/statistics"> <i class="fas fa-hashtag fa-sm"></i> statistics,</a>   <a href="/blog/tag/diffusion"> <i class="fas fa-hashtag fa-sm"></i> diffusion</a>     ·   <a href="/blog/category/statistics"> <i class="fas fa-tag fa-sm"></i> statistics</a>   </p> </header> <article class="post-content"> <p>In Bayes’ rule, evidence is another name for the marginal likelihood of the data. It is called as “evidence”, because it measures how well the data supports the model.</p> \[p(z|x) = \frac{p(x|z)p(z)}{\color{red}{p(x)}}\] <p>where \(p(x) = \int p(x,z) dz = \int p(x\vert z)p(z)dz\).</p> <p>The ultimate goal of many likelihood-based generative models is to maximize this. However, this marginal likelihood is most of the time intractable due to the integral over all possible latent variable \(z\). Therefore, ELBO, which is a tractable bound, comes as a useful tool in designing optimizers. Since we cannot maximize \(\text{log}p_\phi(x)\) directly, we instead maximize it’s lower bound.</p> \[\text{log}p_\phi(x) \geq L_{\text{ELBO}}(\theta, \phi; x)\] <p>I wrote the above formula for VAEs, as \(\theta\) is an encoder parameter, \(\phi\) is a decoder parameter. In case of DDPMs, encoder is fixed, so \(\theta\) doesn’t exist and \(\phi\) is the learnable decoder (denoiser) parameter.</p> <h3 id="deriving-elbo">Deriving ELBO</h3> <p>Deriving the formula for ELBO is actually quite simple. I do it here for VAEs. Let \(q_\theta(z|x)\) be an encoder of a VAE, which is an approximation of \(p_\phi(z|x)\). For DDPMs, this is just replaced by the known encoder \(p(x_{i-1}|x_i)\).</p> <p>First, we rewrite the marginal with a joint distribution and use \(q_\theta\).</p> \[\text{log}p_\phi(x) = \text{log} \int p_\phi(x,z) dz = log \int q_\theta(z|x) \frac{p_\phi(x,z)}{q_\theta(z|x)}dz \\ = \text{log} \mathbb{E}_{z \sim q_\theta(z|x)}[\frac{p_\phi(x,z)}{q_\theta(z|x)}]\] <p>Since log is concave function, the Jensen’s inequality tells us that \(\text{log} \mathbb{E}[Z] \geq \mathbb{E}[\text{log}Z]\) (think about drawing a straight line between two points in a concave function):</p> \[\text{log} \mathbb{E}_{z \sim q_\theta(z|x)}[\frac{p_\phi(x,z)}{q_\theta(z|x)}] \geq \mathbb{E}_{z \sim q_\theta(z|x)}[\text{log} \frac{p_\phi(x,z)}{q_\theta(z|x)}]\] <p>Then, just expanding the last equation gives the ELBO:</p> \[L_{ELBO} = \mathbb{E}_{z \sim q_\theta(z|x)}[\text{log}p_\phi(x|z)] - D_{KL}(q_\theta(z|x) || p(z))\] <p>where the KL divergence is the difference in the information of two distributions</p> \[D_{KL}(q_\theta(z|x) || p(z)) = \mathbb{E}_{z \sim q_\theta(z|x)}[\text{log}\frac{q_\theta(z|x)}{p(z)}].\] <p>The first term of ELBO is the likelihood of observing the data given the latent variable, which in other words, is the reconstruction term. The second KL divergence term pushes the encoder distribution \(q_\theta(z\vert x)\) to be close to the latent distribution \(p(z)\), which in many cases is a simple Gaussian.</p> <h3 id="elbo-for-ddpms">ELBO for DDPMs</h3> <p>Same as VAEs, the DDPMs’ training objective is also to maximize the marginal log-likelihood. Notable differences are that the encoder is a known distribution, since the “noising” process is small steps of adding Gaussian noise, and that the marginal log-likelihood is a joint distribution over all \(T\) annealing steps, instead of a single latent \(z\). Note that the DDPM notation is that \(x_0\) is the clean data and \(x_T\) is a noisy sample.</p> \[log p_\phi(x) = log \int p_\phi(x, x_{0:T})dx_{0:T}.\] <p>In the same way as in VAEs where we inserted \(q_\theta(z\vert x)\), here we insert known distribution of the forward process \(p(x_{0:T}\vert x)\), which is a joint distribution of all the noisy samples of x.</p> \[log p_\phi(x) = log \int p(x_{0:T}|x) \frac{p_\phi(x, x_{0:T})}{p(x_{0:T}|x)}dx_{0:T}\] <p>Expanding the above and using the Jensen’s inequality in the same way will give the following ELBO with 3 terms:</p> \[L_{\text{ELBO}}(x_0; \phi) = - D_{KL}(p(x_T|x_0)||p_{\text{prior}}(x_T)) + \mathbb{E}_{p(x_1|x_0)}[\text{log}p_\phi(x_0|x_1)] - \sum_{i=1}^T \mathbb{E}_{p(x_i|x_0)} [D_{KL}(p(x_{i-1}|x_i, x_0)||p_\phi(x_{i-1}|x_i))]\] <p>where the first one becomes close to zero if we add enough noise to make \(x_T\) completely noisy so that \(p(\cdot \vert x_0) \approx p_{\text{prior}}(\cdot)\), and the second one is a reconstruction/denoising term and the last term is the diffusion term, where we make sure that the true distribution of the denoising process \(p(x_{i-1}\vert x_i)\) is approximated with the learned denoiser \(p_\phi(x_{i-1}\vert x_i)\). Note that since \(p(x_{i-1}\vert x_i)\) is not tractable, conditional distribution is used as done commonly in most diffusion/flow models.</p> <p><strong>References</strong></p> <ul> <li>Lai, Chieh-Hsin, et al. “The principles of diffusion models.” arXiv preprint arXiv:2510.21890 (2025).</li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Kyung Yun Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>